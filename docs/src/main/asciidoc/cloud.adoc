
= [[spring-cloud-paas]] PaaS, or, The Application-Centric Cloud

== Spring Cloud
http://spring.io/projects/spring-cloud[Spring Cloud] is a much larger project than just the ability to consume PaaS components.  The rest of the project provides support for building large, cloud-scale applications using Spring. In particular, Spring Cloud provides the primitives to support a  http://martinfowler.com/articles/microservices.html[microservice] architecture. We will discuss the rest of Spring Cloud in  the Spring Cloud chapte

== What Even Is.. Cloud?

These days, the idea of a _PaaS_ (platform-as-a-service) is a bit, pardon the pun, _cloudy_. Conceptually, a _cloud_ is something that delivers services, on-demand. This definition is (ack..choke! _groan_!) a bit _nebulous_.

It's easiest to define clouds in terms of what they let you, the operator, create.

We all know what _SaaS_ (software-as-a-service) is. It refers to end-user software deployed in a managed environment (like  http://salesforce.com[SalesForce.com] or Google Apps) that is administered (if at all) _within_ the application itself. Thus, a SaaS might make it easy to create more accounts on a hosted CRM, or more e-mail accounts on a hosted cloud-provider.

Many of us recognize _IaaS_ (infrastructure-as-a-service) when we see it. Amazon Web Services and Google Compute Engine are pretty common examples of IaaS. IaaS concern themselves with the running and management of operating system images and relevant resources like disk space, RAM, and network IO. An IaaS might make it easy to create new Linux virtual machines with an arbitrary amount of RAM.

So, what is a PaaS? It's a cloud that concerns itself with the management of deployed applications and services. PaaS is also the hottest, fastest growing sector in the industry. Pivotal's Cloud Foundry, SalesForce's Heroku, Microsoft's Azure, RedHat's OpenShift, Google's App Engine, etc., are all PaaS entries. A PaaS might make it easy to create and deploy more applications, and resources for those applications like message queues, web servers, and databases. A PaaS does not typically expose nuances about the environment in which the application is running. There's no reason to care; it's likely a modern revision of Linux that works well for 99% of application workloads out there.

Beyond the process' environment, most PaaSes don't expose anything to the application, not even a durable filesystem. Instead, applications consume services (like SendGrid for email, Amazon Web Service's S3 for blog storage, RedisLabs for hosted Redis instances, etc.) over the network. The http://12factor.net[12 Factor application manifesto] refers to these as _backing services_.

Until recently, this meant that a developer would describe in some declarative fashion via a CLI or a configuration file the resources an application needs. This configuration typically included parameters like how much RAM should be assigned, the HTTP routes to expose the application under, how many instances of an application should be run, and what backing services (databases, message queues) an application needs.

In order for the PaaS to run applications with these parameters, the PaaS had to understand the nature of the applications it runs. After all, most modern PaaSes treat everything as processes to be initialized, started, scaled, and stopped. They don't care that a Java application might need an Apache Tomcat, or that a PHP application might need an Apache HTTPD or Nginx installation.

Hopefully, this intuition about applications is an extension plane for developers, and not baked into the platform. Heroku and Cloud Foundry, for example, both use _buildpacks_ to adapt given application artifacts to the lifecycle callbacks of the platform. Most PaaS take application artifacts as inputs. A PaaS like Cloud Foundry expects a `.jar` or `.war`, for example. The default (overridable) Java buildpack does things like install the right supporting libraries, setup the right `PATH`, and start and stop the right infrastructure (like Apache Tomcat) and do so on the network port provided by the platform.

This approach works well: the rigidity creates a consistency in development and operations. Developers need only make their application behave appropriately with the buildpack (or override it!) and then reuse the recipe. As long as the PaaS is happy with the input applications, then almost everything required to move an application from a developer's machine to a production, or production-like, environment can be done with almost no administration. This makes operations happy because it frees them to focus on more important things, and it makes developers happy because they're not waiting for operations to catch up. (No more waiting days or weeks for operations to provision a machine and setup security!)

== Cloud Foundry
== Heroku
== Microsoft Azure
==  Lattice, a Distributed Runtime for your Containerized Spring Workloads

### Portable Applications

Spring has always tried to make application portability as easy as possible. It supports good design patterns such as those prescribed by the http://12factor.net[12 Factor App] manifesto.

As developers, we're used to being able to test applications in isolation, to validate the inputs into an application and validate the resulting behavior.  We're used to reproducible builds; we're used to being able to throw away the build and - with the disciplined use of tags and so on - reproduce the same build. This isn't news.

### Cattle

It took us developers a while to get to this place, but we did. In the last 8 years we've seen this rigor come to operations teams. Operations want reproducible infrastructure as much as we developers want reproducible builds. Consistency and reproducibility are even more important with disposable, ephemeral, cloud infrastructure. As Adrian Cockcroft (formerly at Netflix) reminds us: treat servers like cattle, not pets.

It's 2015, things have come a long way. Developers and operations are - ideally at least - two highly integrated teams. Developers want consistency in the way their applications run, operations want consistency in their infrastructure.

### Deploy Containers

[Linux Containers, or _LXC_](http://wikipedia.com/wiki/LXC), are a set of features in the Linux kernel designed to isolate applications. Applications run as though they've got the run of the kernel. LXC aren't, then, a single API or feature, but a set of independant ones that can be used together.  Container technologies like Docker and Rocket leverage LXC features and let you control them declaratively. Using Docker, for example, you can write a Dockerfile, check it into your code and then use that Docker file and code to reproduce both the application and its environment, and do so at a much lower runtime footprint than traditional virtualization. This makes things simpler: operations deploy containers, not applications. To learn more about [using Spring Boot and Docker, check out this handy guide](https://spring.io/guides/gs/spring-boot-docker/).

The only thing that remains, then, is automation around managing, running and scaling containerized workloads. This is where a distributed runtime like [Lattice](http://lattice.cf) comes in. From the site:

> Lattice aspires to make clustering containers easy. Lattice includes a cluster scheduler, http load balancing, log aggregation and health management. Lattice containers can be long running or temporary tasks which get dynamically scaled and balanced across a cluster. Lattice packages components from Cloud Foundry to provide a cloud native platform for individual developers and small teams.

Lattice is easy [to deploy both on a proper cluster](https://github.com/cloudfoundry-incubator/lattice#clustered-deployment) or [locally using Vagrant](https://github.com/cloudfoundry-incubator/lattice#local-deployment).

### A Simple Spring Boot Application

### The Code
Let's get a simple example working. Our first example will be a basic Spring Boot CLI REST endpoint with no services in a file, `service.groovy`:

.A Spring Boot CLI service
[source,java]
----
include::{book-root}/bootiful-cloud/lattice/service.groovy[]
----

We have two endpoints (`/hello`, and `/hello/{name}`) that simply return a value, and a third endpoint (`/killme`) that will abruptly (rudely!) terminate the endpoint. To run this on your local machine, use the [Spring Boot CLI](http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started-installing-the-cli):

```bash
spring run service.groovy
```

The goal is to get something we can wrap in a Docker container, and the simpler a Docker file, the better. Let's deploy a standalone `.jar`:

```bash
mkdir -p build
spring jar build/service.jar ./service.groovy
```

This will create a `.jar` in the `build` directory that we can run normally:

```java
java -jar build/service.jar
```

### The Docker Image

Lattice needs a reference to a Docker image, as deployed into a Docker image repository. There are private repositories available, but let's just use the publicly available Docker Hub.  If you haven't already created an account, [do so on the Docker Hub website](https://hub.docker.com/account/signup/).

We'll need to describe our application with a `Dockerfile`, as well:

```docker
FROM java
ADD ./build/service.jar /service.jar
ADD ./build/run.sh /run.sh
RUN chmod a+x /run.sh
CMD /run.sh
```

This file lives in the same directory as `service.groovy`. It tells Docker to use the `java` Docker image as the base. Docker images are made of layers, and - ideally - you should be able to reuse as much as possible of somebody else's layers. In this case, we're reusing [the standard `java` image](https://registry.hub.docker.com/_/java/). This image already has Linux and Java loaded in it.  We just need to derive from it and specialize it so that it knows about our application. The `ADD` directives _mount_ the resources from the local filesystem in the Docker image filesystem's root (`/`). The `RUN` directive tells Docker what we want done while building the image itself to customize the files that have been added. The `CMD` directive gives Docker a default command to run when the container starts up. In this case, it'll run `/run.sh`, which looks like this:

```bash
#!/bin/bash
java -jar /service.jar
```

With all of this in place, we can build and deploy a Docker image. To keep things simple, I use a Bash function to automatically build and deploy my Docker image:

```bash
function build_docker_image(){
  curdir=`dirname $0`
  target=$curdir/$1
  app=$3
  user=$2
  cp $curdir/run.sh $target
  docker build -t $app $curdir
  docker tag -f $app $user/$app
  docker push $user/$app
}
```

Then, I invoke the function inside of the same `.sh` script as follows:

```bash
build_docker_image build bootiful-docker starbuxman
```

I can now easily point Lattice to my deployed Docker image (`starbuxman/bootiful-docker`).

### Deploying to Lattice
Make sure that [Lattice is up and running](http://lattice.cf/docs/getting-started/) and that you have the `ltc` CLI installed. The following Bash function will deploy your container to Lattice, make sure there are 5 instances of the application running, and then show informaion related to what applications are runninng (a bit like `ps aux`) and then show specific state for our application.

```bash
function deploy_to_lattice(){
  app=$2
  user=$1

  ltc rm $APP
  ltc create $APP $user/$APP -- /run.sh
  ltc scale $app 2

  ltc list
  ltc status $app
}
```

This function removes the existing app, if it's available, creates a new one (in this case, named `bootiful-docker`), then scales that application to have 5 concurrent running instances. Finally, `ltc list` is sort of like `ps aux` for Lattice - it'll display all the running processes. `ltc status` gives specific information about our deployed application.

Use it as follows:

```bash
deploy_to_lattice starbuxman bootiful-docker
```

You'll see output on the shell confirming that the application has been run, like this:

```bash
~/D/b/simple-example git:experiment ❯❯❯ ltc list                                                                                                     ✱
App Name			Instances	DiskMB		MemoryMB	Route
bootiful-docker			2/2		1024		128		bootiful-docker.192.168.11.11.xip.io, bootiful-docker-8080.192.168.11.11.xip.io => 8080
..
```

Visit `http://bootiful-docker.192.168.11.11.xip.io/hello/Lattice` to see the output of the REST endpoint. Visit `http://bootiful-docker.192.168.11.11.xip.io/env` (which comes from Spring Boot's Actuator module) to see the enumeration of the environment in which the application's running. Finally, visit http://bootiful-docker.192.168.11.11.xip.io/killme`
 to kill an instance. This will cause an instance of the application to exit. Lattice will immediately restart the instance. If you cause an instance of lattice-app to exit repeatedly Lattice will eventually start applying a backoff policy and restart the instance only after increasing intervals of time (30s, 60s, etc..).

The application has access to interesting environment variables like `CF_INSTANCE_IP`, `CF_INSTANCE_PORT`, which tell the running application the IP address and port used to address the containerized application from the outside. To learn more about this, [check out the docs on Lattice's environment](https://github.com/cloudfoundry-incubator/receptor/blob/master/doc/environment.md)

### Deploying and Consuming a Backing Service with Lattice

Thus far we've just deployed an HTTP service. Lattice handily supports all manner of containerized workloads. It's worth noting that the routes don't work for non HTTP traffic. Only TCP. If you want to to talk to a node, you'll need to use its IP address, directly. You can retreive the IP using the aforementioned environment variables, or just use `ltc status $APP_NAME`.


Let's standup PostgreSQL. There are any number of readily available, containerized infrastructure available for the taking on Docker Hub. Just find one and then deploy it to Lattice, like this:

```bash
ltc create --run-as-root bootiful-docker-postgres postgres
```
This will launch the [PostgreSQL Docker image](https://registry.hub.docker.com/_/postgres/) from Docker Hub. You can customize the running PostgreSQL instance by passing in environment variables, like this:

```bash
ltc create --run-as-root --env "POSTGRES_PASSWORD=pw" bds postgres
```

On my machine, `ltc status` yeilded the following:

```bash
~ ❯❯❯ ltc status bootiful-docker-postgres
================================================================================
      bootiful-docker-postgres
--------------------------------------------------------------------------------
Instances	1/1
Stack		lucid64
Start Timeout	0
DiskMB		1024
MemoryMB	128
CPUWeight	100
Ports		  5432
Routes		bootiful-docker-postgres.192.168.11.11.xip.io => 5432
		      bootiful-docker-postgres-5432.192.168.11.11.xip.io => 5432
--------------------------------------------------------------------------------
Environment

POSTGRES_PASSWORD="pw"
PORT="5432"

================================================================================
      Instance 0  [RUNNING]
--------------------------------------------------------------------------------
InstanceGuid	5b6e33a5-79f1-4311-4a2c-ba7108063fb1
Cell ID		lattice-cell-01
Ip		192.168.11.11
Port Mapping	61002:5432
Since		2015-04-03 10:57:48 (PDT)
Crash Count 	0
--------------------------------------------------------------------------------

```

I could then access the PostgreSQL instance like this:
```bash
psql -U postgres -h 192.168.11.11 -p 61002 postgres
```
== Using the Docker Maven Plugin to Generate Dockerfiles from Maven

== Cloud-Ready Session Management with Spring Session
